import os
from langchain_openai import OpenAIEmbeddings
from openai import OpenAI
from langchain_community.chat_models import ChatOpenAI
from langchain_core.runnables import Runnable, RunnableMap, RunnableLambda, RunnablePassthrough
from langchain.memory import ConversationBufferMemory
from operator import itemgetter
from dotenv import load_dotenv
load_dotenv()

def generate_chatbot_tempalte(query, records):
    record = ""
    for index,r in enumerate(records):
        record += f"""
                    - **Record : {index + 1}**
                    - **Company:** {r['CompanyName']}
                    - **Auditor Company:** {r['AuditorName']}
                    - **Report Name:** {r['ReportName']}
                    - **Report Text:** {r['ReportText']}
                """
    prompt_template = f"""
    # NumInformatics 10-K Analyzer Chatbot

    ## Motive
    The motive of the NumInformatics 10-K Analyzer Chatbot is to assist users in generating accurate and relevant answers based on the audit reports of companies. By leveraging similarity search through documents, the chatbot aims to provide reliable information and insights, ensuring users have access to the data they need.

    ## Chatbot Name
    AuditInsight Bot

    ## Version
    1.2.0

    ## Instructions
    You are the NumInformatics 10-K Analyzer Chatbot, an assistant designed to generate answers based on the top two similarity searches through the documents provided. Your task is to carefully and accurately analyze the given information and respond to the user's query.

    ### Steps to Follow:
    1. **Understand the Context:**
    - **Chat History:** You will be given a history of the conversation which consists of queries from the `user` and responses generated by `ai` (you).
    - **Current Query:** Here is the current query from the `user`: {query}.

    2. **Analyze the Records:**
    - You will be provided with two similar records that match the query. Each record consists of:
        - **Company:** The company being audited.
        - **Auditor Company:** The company conducting the audit.
        - **Report Name:** The name of the audit report.
        - **Report Text:** The audit report generated by the `Auditor` on the `Company`.
    - If records are not provided please do not assume or hallucinate any records just say: "Lack of evidence". 

    3. **Generate an Answer:**
    - Use the information in the records to generate an answer to the user's query.
    - If the records are not provided or do not contain the solution or relevant information related to the query, respond with: "I cannot answer due to lack of evidence." Do not hallucinate or provide irrelevant information.

    ### Records:
    {record}

    Follow these instructions carefully to provide the most accurate and relevant response to the user's query. Please do not generate/hallucinate any records.

    """

    return prompt_template

class OpenAIChatResponse:
    def __init__(self, **kwargs):
        self.openai_api_key = os.environ.get("OPENAI_API_KEY")
        self.client = OpenAI()
    
    def generate_response(self, history, query:str, record, model:str = "gpt-3.5-turbo", max_token:int = 4000):
        
        memory = self.make_memory_from_testing_chat_history(history)

        loaded_memory = RunnablePassthrough.assign(
            chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter("history"),
        )

        model = ChatOpenAI(temperature=0.3, model=model, max_tokens=max_token)

        # Handle the prompt
        handle_prompt = RunnableLambda(
            lambda inputs: inputs["prompt"]
        )

        intent_clf_chain = loaded_memory | handle_prompt| model

        prompt = generate_chatbot_tempalte(query, record)
    
        result = intent_clf_chain.invoke({"prompt" : prompt})
        
        print("\n\n\n\n===================={full_response}====================\n\n\n")
        print(result)

        print("\n\n\n\n===================={response_content}====================\n\n\n")
        print(result.content)

        print("\n\n\n\n===================={response_metadata}====================\n\n\n")
        print(result.response_metadata)

        # try:
        #     chat_completion = self.client.chat.completions.create(
        #         messages=[
        #             {
        #                 "role": "user",
        #                 "content": generate_chatbot_tempalte(history, query, record),
        #             }
        #         ],
        #         model=model,
        #         max_tokens=max_token,
        #     )
        #     return chat_completion.choices[0].message.content
        # except:
        #     print('Error generating response')
        #     return None
    
    def make_memory_from_testing_chat_history(self,chat_history):
        """
        This function is used to convert the chatHistory generated by testing UI (tabot_chatbot_UI) 
        Generates a ConversationBufferMemory object from a chat history for chatbot_test_UI.
        
        Args:
        - chat_history (list): A list of dictionaries, each containing a message from 'user' and a response from 'ai'.
        Example: [{'user': 'Hello, how are you?', 'ai': 'I am fine, thank you.'}, ...]
        
        Returns:
        - ConversationBufferMemory: The memory object containing the conversation history.
        """
        memory = ConversationBufferMemory(
            return_messages=True,
            output_key="answer", 
            input_key="question"
        )
        
        if len(chat_history) == 0:
            return memory

        for i in range(len(chat_history)):
            user_message = chat_history[i]['user']
            ai_response = chat_history[i]['ai']

            if user_message is not None:
                memory.chat_memory.add_user_message(user_message)
            if ai_response is not None:
                memory.chat_memory.add_ai_message(ai_response)

        return memory

    def generate_summary(self, text:str, model:str = "gpt-3.5-turbo", max_token:int = 4000):
        query = f"""
                    Please generate a detailed summary of the following text: {text}
                """
        try:
            chat_completion = self.client.chat.completions.create(
                messages=[
                    {
                        "role": "user",
                        "content": query,
                    }
                ],
                model=model,
                max_tokens=max_token,
            )
            return chat_completion.choices[0].message.content
        except:
            print('Error generating response')
            return None

class OpenAIEmbedder:
    def __init__(self, model:str='text-embedding-ada-002', **kwargs):
        self.model = model  # can also be text-embedding-3-large        
        self.openai_api_key = os.environ.get("OPENAI_API_KEY")

        self.embedder = OpenAIEmbeddings(
            model=self.model,  #'text-embedding-ada-002'
            openai_api_key=self.openai_api_key,
            **kwargs
        )
        
    def get_embedder(self):
        return self.embedder
    
    def embed_text(self, text:str):
        return self.embedder.embed_query(text)
    
    def embed_query(self, query_text:str):
        return self.embedder.embed_query(query_text)

    def embed_documents(self, docs:list[str]):
        return self.embedder.embed_documents(docs)
    


if __name__ == "__main__":
    # Step 1: Create the memory object
    history = [
        {'user': 'Hello, how are you?', 'ai': 'I am fine, thank you.'}
    ]

    ai = OpenAIChatResponse()
    ai.generate_response(history=history, query="Can you give me details about compay: Apple Inc.", record=[])